# Docker Images & Container Builds - Transformers Implementation\n\n## Overview\nComplete Docker container specifications for Boardroom TEE using Transformers library for LLM model loading and inference across hub and spoke agents.\n\n---\n\n## Base Agent Framework Container\n\n### Shared Base Image\n```dockerfile\n# base-agent/Dockerfile\nFROM python:3.11-slim\n\n# Install system dependencies\nRUN apt-get update && apt-get install -y \\\n    curl \\\n    wget \\\n    git \\\n    build-essential \\\n    && rm -rf /var/lib/apt/lists/*\n\n# Set working directory\nWORKDIR /app\n\n# Install Python dependencies\nCOPY requirements-base.txt .\nRUN pip install --no-cache-dir -r requirements-base.txt\n\n# Create app structure\nRUN mkdir -p /app/src /app/data /app/logs /app/crypto /app/config /app/models\n\n# Copy shared components\nCOPY src/shared/ /app/src/shared/\nCOPY src/attestation/ /app/src/attestation/\nCOPY src/communication/ /app/src/communication/\n\n# Set environment variables\nENV PYTHONPATH=/app/src\nENV TRANSFORMERS_CACHE=/app/models\nENV HF_HOME=/app/models\n\n# Create non-root user for security\nRUN useradd -m -u 1000 agent && chown -R agent:agent /app\nUSER agent\n\nCMD [\"python\", \"-c\", \"print('Base agent framework ready')\"]\n```\n\n### Base Requirements\n```txt\n# requirements-base.txt\ntransformers==4.36.0\ntorch==2.1.0\nnumpy==1.24.3\nfastapi==0.104.1\nuvicorn==0.24.0\nrequests==2.31.0\naiohttp==3.9.0\ncryptography==41.0.7\npydantic==2.5.0\nsqlite3-utils==3.35.0\npython-multipart==0.0.6\njinja2==3.1.2\n\n# Document processing\nPyPDF2==3.0.1\npython-docx==0.8.11\npandas==2.1.3\nopenpyxl==3.1.2\n\n# TEE and attestation\nbase64\nhashlib\njson\ndatetime\nlogging\n```\n\n---\n\n## Hub Container (Llama-3.2-1B-Instruct)\n\n### Hub Dockerfile\n```dockerfile\n# hub/Dockerfile\nFROM boardroom-base-agent:latest\n\n# Switch to root for installation\nUSER root\n\n# Install hub-specific dependencies\nCOPY requirements-hub.txt .\nRUN pip install --no-cache-dir -r requirements-hub.txt\n\n# Download and cache Llama-3.2-1B-Instruct model\nRUN python -c \"\nimport os\nos.environ['HF_HOME'] = '/app/models'\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nimport torch\n\nprint('Downloading Llama-3.2-1B-Instruct...')\ntokenizer = AutoTokenizer.from_pretrained('meta-llama/Llama-3.2-1B-Instruct')\nmodel = AutoModelForCausalLM.from_pretrained(\n    'meta-llama/Llama-3.2-1B-Instruct',\n    torch_dtype=torch.float16,\n    device_map='cpu',\n    low_cpu_mem_usage=True\n)\nprint('Model cached successfully')\ndel model\ndel tokenizer\n\"\n\n# Copy hub-specific code\nCOPY src/hub/ /app/src/hub/\nCOPY config/hub_config.yaml /app/config/\n\n# Set hub environment variables\nENV HUB_MODEL_NAME=meta-llama/Llama-3.2-1B-Instruct\nENV HUB_MAX_MEMORY_MB=3000\nENV HUB_API_PORT=8080\nENV HUB_ATTESTATION_PORT=29343\n\n# Switch back to agent user\nRUN chown -R agent:agent /app\nUSER agent\n\n# Health check\nHEALTHCHECK --interval=30s --timeout=10s --start-period=40s --retries=3 \\\n  CMD curl -f http://localhost:8080/health || exit 1\n\n# Start hub service\nCMD [\"python\", \"/app/src/hub/main.py\"]\n```\n\n### Hub Requirements\n```txt\n# requirements-hub.txt\n# Document processing specific\nPyPDF2==3.0.1\npython-docx==0.8.11\npandas==2.1.3\nopenpyxl==3.1.2\nemail-validator==2.1.0\n\n# LLM processing\ntorch==2.1.0\ntransformers==4.36.0\naccelerate==0.24.1\n\n# Database and storage\nsqlite3\npsutil==5.9.6\n\n# Background processing\ncelery==5.3.4\nredis==5.0.1\n```\n\n### Hub Main Application\n```python\n# src/hub/main.py\nimport asyncio\nimport logging\nfrom fastapi import FastAPI, HTTPException\nfrom contextlib import asynccontextmanager\n\nfrom src.shared.models import HubConfig\nfrom src.hub.services.llm_manager import UnifiedHubLLM\nfrom src.hub.services.document_processor import DocumentProcessor\nfrom src.hub.services.agent_orchestrator import AgentOrchestrator\nfrom src.hub.services.attestation_discovery import AttestationDiscoveryService\nfrom src.hub.api.routes import router\n\n# Global services\nhub_llm = None\ndocument_processor = None\nagent_orchestrator = None\nattestation_service = None\n\n@asynccontextmanager\nasync def lifespan(app: FastAPI):\n    # Startup\n    global hub_llm, document_processor, agent_orchestrator, attestation_service\n    \n    logging.info(\"Starting Boardroom TEE Hub...\")\n    \n    # Initialize LLM\n    hub_llm = UnifiedHubLLM()\n    await hub_llm.load_model()\n    \n    # Initialize services\n    document_processor = DocumentProcessor(hub_llm)\n    agent_orchestrator = AgentOrchestrator(hub_llm)\n    attestation_service = AttestationDiscoveryService()\n    \n    logging.info(\"Hub services initialized successfully\")\n    \n    yield\n    \n    # Shutdown\n    logging.info(\"Shutting down hub services...\")\n    if hub_llm:\n        hub_llm.cleanup_memory()\n\n# Create FastAPI app\napp = FastAPI(\n    title=\"Boardroom TEE Hub\",\n    description=\"Central hub for document processing and agent orchestration\",\n    version=\"1.0.0\",\n    lifespan=lifespan\n)\n\n# Include API routes\napp.include_router(router, prefix=\"/api/v1\")\n\n@app.get(\"/health\")\nasync def health_check():\n    return {\n        \"status\": \"healthy\",\n        \"model_loaded\": hub_llm.is_loaded if hub_llm else False,\n        \"memory_usage\": hub_llm.get_memory_usage() if hub_llm else None\n    }\n\nif __name__ == \"__main__\":\n    import uvicorn\n    uvicorn.run(\n        \"main:app\",\n        host=\"0.0.0.0\",\n        port=8080,\n        log_level=\"info\"\n    )\n```\n\n---\n\n## Finance Agent Container (AdaptLLM/Finance-LLM-7B)\n\n### Finance Agent Dockerfile\n```dockerfile\n# spoke_finance/Dockerfile\nFROM boardroom-base-agent:latest\n\n# Switch to root for installation\nUSER root\n\n# Install finance-specific dependencies\nCOPY requirements-finance.txt .\nRUN pip install --no-cache-dir -r requirements-finance.txt\n\n# Download and cache AdaptLLM/Finance-LLM-7B model\nRUN python -c \"\nimport os\nos.environ['HF_HOME'] = '/app/models'\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nimport torch\n\nprint('Downloading AdaptLLM/Finance-LLM-7B...')\ntokenizer = AutoTokenizer.from_pretrained('AdaptLLM/finance-LLM')\nmodel = AutoModelForCausalLM.from_pretrained(\n    'AdaptLLM/finance-LLM',\n    torch_dtype=torch.float16,\n    device_map='cpu',\n    low_cpu_mem_usage=True\n)\nprint('Finance model cached successfully')\ndel model\ndel tokenizer\n\"\n\n# Copy finance agent code\nCOPY src/agents/finance/ /app/src/agents/finance/\nCOPY config/finance_config.yaml /app/config/\n\n# Set finance agent environment variables\nENV AGENT_TYPE=finance\nENV AGENT_MODEL_NAME=AdaptLLM/finance-LLM\nENV AGENT_API_PORT=8081\nENV AGENT_ATTESTATION_PORT=29344\nENV AGENT_MAX_MEMORY_MB=7000\n\n# Switch back to agent user\nRUN chown -R agent:agent /app\nUSER agent\n\n# Health check\nHEALTHCHECK --interval=30s --timeout=10s --start-period=60s --retries=3 \\\n  CMD curl -f http://localhost:8081/health || exit 1\n\n# Start finance agent\nCMD [\"python\", \"/app/src/agents/finance/main.py\"]\n```\n\n### Finance Requirements\n```txt\n# requirements-finance.txt\n# Financial analysis libraries\nnumpy==1.24.3\npandas==2.1.3\nscipy==1.11.4\nscikit-learn==1.3.2\n\n# Financial calculations\nQuantLib-Python==1.32\npyfolio==0.9.2\nstatsmodels==0.14.0\n\n# LLM processing (7B model)\ntorch==2.1.0\ntransformers==4.36.0\naccelerate==0.24.1\nbitsandbytes==0.41.3  # For quantization if needed\n\n# Financial data processing\nyfinance==0.2.28\npandas-datareader==0.10.0\n```\n\n### Finance Agent Main\n```python\n# src/agents/finance/main.py\nimport asyncio\nimport logging\nfrom fastapi import FastAPI\nfrom contextlib import asynccontextmanager\n\nfrom src.shared.models import AgentConfig\nfrom src.agents.finance.services.finance_llm import FinanceLLM\nfrom src.agents.finance.services.financial_analyzer import FinancialAnalyzer\nfrom src.attestation.attestation_client import SpokeAttestationClient\nfrom src.communication.secure_communication import SecureInterAgentCommunication\nfrom src.agents.finance.api.routes import router\n\n# Global services\nfinance_llm = None\nfinancial_analyzer = None\nattestation_client = None\nsecure_comm = None\n\n@asynccontextmanager\nasync def lifespan(app: FastAPI):\n    # Startup\n    global finance_llm, financial_analyzer, attestation_client, secure_comm\n    \n    logging.info(\"Starting Finance Agent...\")\n    \n    # Initialize Finance LLM\n    finance_llm = FinanceLLM(model_name=\"AdaptLLM/finance-LLM\")\n    await finance_llm.load_model()\n    \n    # Initialize services\n    financial_analyzer = FinancialAnalyzer(finance_llm)\n    attestation_client = SpokeAttestationClient(\n        agent_id=\"finance-agent\",\n        hub_discovery_endpoint=\"https://hub:8090/discovery\"\n    )\n    secure_comm = SecureInterAgentCommunication(attestation_client)\n    \n    # Register with hub\n    await attestation_client.register_self_with_hub()\n    \n    logging.info(\"Finance agent initialized successfully\")\n    \n    yield\n    \n    # Shutdown\n    logging.info(\"Shutting down finance agent...\")\n    if finance_llm:\n        finance_llm.cleanup_memory()\n\n# Create FastAPI app\napp = FastAPI(\n    title=\"Boardroom TEE Finance Agent\",\n    description=\"Specialized financial analysis agent\",\n    version=\"1.0.0\",\n    lifespan=lifespan\n)\n\n# Include API routes\napp.include_router(router, prefix=\"/api/v1\")\n\n@app.get(\"/health\")\nasync def health_check():\n    return {\n        \"status\": \"healthy\",\n        \"agent_type\": \"finance\",\n        \"model_loaded\": finance_llm.is_loaded if finance_llm else False,\n        \"attestation_verified\": attestation_client.is_verified if attestation_client else False\n    }\n\nif __name__ == \"__main__\":\n    import uvicorn\n    uvicorn.run(\n        \"main:app\",\n        host=\"0.0.0.0\",\n        port=8081,\n        log_level=\"info\"\n    )\n```\n\n---\n\n## Marketing Agent Container (Mistral-7B-Instruct-v0.3)\n\n### Marketing Agent Dockerfile\n```dockerfile\n# spoke_marketing/Dockerfile\nFROM boardroom-base-agent:latest\n\n# Switch to root for installation\nUSER root\n\n# Install marketing-specific dependencies\nCOPY requirements-marketing.txt .\nRUN pip install --no-cache-dir -r requirements-marketing.txt\n\n# Download and cache Mistral-7B-Instruct-v0.3 model\nRUN python -c \"\nimport os\nos.environ['HF_HOME'] = '/app/models'\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nimport torch\n\nprint('Downloading Mistral-7B-Instruct-v0.3...')\ntokenizer = AutoTokenizer.from_pretrained('mistralai/Mistral-7B-Instruct-v0.3')\nmodel = AutoModelForCausalLM.from_pretrained(\n    'mistralai/Mistral-7B-Instruct-v0.3',\n    torch_dtype=torch.float16,\n    device_map='cpu',\n    low_cpu_mem_usage=True\n)\nprint('Marketing model cached successfully')\ndel model\ndel tokenizer\n\"\n\n# Copy marketing agent code\nCOPY src/agents/marketing/ /app/src/agents/marketing/\nCOPY config/marketing_config.yaml /app/config/\n\n# Set marketing agent environment variables\nENV AGENT_TYPE=marketing\nENV AGENT_MODEL_NAME=mistralai/Mistral-7B-Instruct-v0.3\nENV AGENT_API_PORT=8082\nENV AGENT_ATTESTATION_PORT=29345\nENV AGENT_MAX_MEMORY_MB=4000\n\n# Switch back to agent user\nRUN chown -R agent:agent /app\nUSER agent\n\n# Health check\nHEALTHCHECK --interval=30s --timeout=10s --start-period=60s --retries=3 \\\n  CMD curl -f http://localhost:8082/health || exit 1\n\n# Start marketing agent\nCMD [\"python\", \"/app/src/agents/marketing/main.py\"]\n```\n\n### Marketing Requirements\n```txt\n# requirements-marketing.txt\n# Customer analytics libraries\nnumpy==1.24.3\npandas==2.1.3\nscipy==1.11.4\nscikit-learn==1.3.2\n\n# Marketing analytics\nplotly==5.17.0\nseaborn==0.13.0\nmatplotlib==3.8.2\n\n# Customer behavior analysis\nlifetimes==0.11.3\nsurprise==1.1.3  # Recommendation systems\nnltk==3.8.1\ntextblob==0.17.1\n\n# LLM processing (7B model)\ntorch==2.1.0\ntransformers==4.36.0\naccelerate==0.24.1\n\n# Social media and web data\nrequests==2.31.0\nbeautifulsoup4==4.12.2\n```\n\n---\n\n## Shared Components\n\n### Unified LLM Manager\n```python\n# src/shared/llm_manager.py\nimport torch\nimport gc\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nfrom typing import Dict, Optional, Tuple\nimport logging\n\nclass BaseLLMManager:\n    \"\"\"Base class for LLM management across all agents\"\"\"\n    \n    def __init__(self, model_name: str, max_memory_mb: int = 4000):\n        self.model_name = model_name\n        self.max_memory_mb = max_memory_mb\n        self.model = None\n        self.tokenizer = None\n        self.is_loaded = False\n        \n    async def load_model(self) -> bool:\n        \"\"\"Load model with memory optimization\"\"\"\n        try:\n            logging.info(f\"Loading model: {self.model_name}\")\n            \n            # Load tokenizer\n            self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)\n            \n            # Load model with optimization\n            self.model = AutoModelForCausalLM.from_pretrained(\n                self.model_name,\n                torch_dtype=torch.float16,\n                device_map=\"cpu\",\n                low_cpu_mem_usage=True,\n                use_cache=False  # Save memory\n            )\n            \n            # Set padding token\n            if self.tokenizer.pad_token is None:\n                self.tokenizer.pad_token = self.tokenizer.eos_token\n            \n            self.is_loaded = True\n            logging.info(f\"Model loaded successfully: {self.model_name}\")\n            return True\n            \n        except Exception as e:\n            logging.error(f\"Failed to load model {self.model_name}: {str(e)}\")\n            return False\n    \n    def get_model(self) -> Tuple[Optional[object], Optional[object]]:\n        \"\"\"Get loaded model and tokenizer\"\"\"\n        if not self.is_loaded:\n            raise RuntimeError(\"Model not loaded. Call load_model() first.\")\n        return self.model, self.tokenizer\n    \n    def cleanup_memory(self):\n        \"\"\"Clean up model memory\"\"\"\n        if self.model:\n            del self.model\n            del self.tokenizer\n            self.model = None\n            self.tokenizer = None\n            self.is_loaded = False\n            gc.collect()\n            torch.cuda.empty_cache() if torch.cuda.is_available() else None\n    \n    def get_memory_usage(self) -> Dict:\n        \"\"\"Get current memory usage\"\"\"\n        import psutil\n        process = psutil.Process()\n        memory_mb = process.memory_info().rss / 1024 / 1024\n        \n        return {\n            \"model_loaded\": self.is_loaded,\n            \"model_name\": self.model_name,\n            \"memory_usage_mb\": memory_mb,\n            \"memory_limit_mb\": self.max_memory_mb,\n            \"memory_utilization\": memory_mb / self.max_memory_mb\n        }\n```\n\n### Attestation Client\n```python\n# src/attestation/attestation_client.py\nimport aiohttp\nimport json\nimport logging\nfrom typing import Dict, Optional\nfrom cryptography.hazmat.primitives import serialization\n\nclass SpokeAttestationClient:\n    \"\"\"Attestation verification client for spoke agents\"\"\"\n    \n    def __init__(self, agent_id: str, hub_discovery_endpoint: str):\n        self.agent_id = agent_id\n        self.hub_endpoint = hub_discovery_endpoint\n        self.is_verified = False\n        self.attestation_data = None\n        \n        # Load TEE keys\n        self.load_tee_keys()\n    \n    def load_tee_keys(self):\n        \"\"\"Load TEE-generated keys\"\"\"\n        try:\n            with open('/app/crypto/privkey.pem', 'rb') as f:\n                self.private_key = serialization.load_pem_private_key(\n                    f.read(), password=None\n                )\n            \n            with open('/app/crypto/pubkey.pem', 'rb') as f:\n                self.public_key_pem = f.read().decode()\n            \n            with open('/app/crypto/quote.txt', 'r') as f:\n                self.attestation_quote = f.read().strip()\n                \n            logging.info(\"TEE keys loaded successfully\")\n            \n        except Exception as e:\n            logging.error(f\"Failed to load TEE keys: {str(e)}\")\n            raise\n    \n    async def register_self_with_hub(self) -> bool:\n        \"\"\"Register this agent's attestation with hub discovery\"\"\"\n        try:\n            registration_data = {\n                \"agent_id\": self.agent_id,\n                \"agent_type\": self.agent_id.split('-')[0],\n                \"attestation_quote\": self.attestation_quote,\n                \"public_key\": self.public_key_pem\n            }\n            \n            async with aiohttp.ClientSession() as session:\n                async with session.post(\n                    f\"{self.hub_endpoint}/register\",\n                    json=registration_data,\n                    headers={\"Content-Type\": \"application/json\"}\n                ) as response:\n                    if response.status == 201:\n                        result = await response.json()\n                        self.is_verified = result.get('verification_status') == 'verified'\n                        logging.info(f\"Agent registered successfully: {self.agent_id}\")\n                        return True\n                    else:\n                        logging.error(f\"Registration failed: {response.status}\")\n                        return False\n                        \n        except Exception as e:\n            logging.error(f\"Registration error: {str(e)}\")\n            return False\n    \n    async def verify_peer_attestation(self, peer_agent_id: str, \n                                     attestation_quote: str) -> bool:\n        \"\"\"Verify another spoke's attestation directly\"\"\"\n        # Implementation would use existing secretGPT attestation logic\n        # Adapted from F:/coding/secretGPT_attestai_1.0/services/attestation_hub/\n        pass\n```\n\n---\n\n## Build Scripts\n\n### Docker Compose Build\n```yaml\n# docker-compose.build.yaml\nversion: '3.8'\n\nservices:\n  # Build base image first\n  base-agent:\n    build:\n      context: .\n      dockerfile: base-agent/Dockerfile\n    image: boardroom-base-agent:latest\n    command: echo \"Base image built\"\n  \n  # Build hub\n  hub:\n    build:\n      context: .\n      dockerfile: hub/Dockerfile\n    image: hub-boardroom-tee:latest\n    depends_on:\n      - base-agent\n  \n  # Build finance agent\n  finance-agent:\n    build:\n      context: .\n      dockerfile: spoke_finance/Dockerfile\n    image: finance-boardroom-tee:latest\n    depends_on:\n      - base-agent\n  \n  # Build marketing agent\n  marketing-agent:\n    build:\n      context: .\n      dockerfile: spoke_marketing/Dockerfile\n    image: marketing-boardroom-tee:latest\n    depends_on:\n      - base-agent\n```\n\n### Build Script\n```bash\n#!/bin/bash\n# build.sh\n\nset -e\n\necho \"Building Boardroom TEE Docker Images...\"\n\n# Build base image first\necho \"Building base agent framework...\"\ndocker build -t boardroom-base-agent:latest -f base-agent/Dockerfile .\n\n# Build hub\necho \"Building hub container...\"\ndocker build -t hub-boardroom-tee:latest -f hub/Dockerfile .\n\n# Build finance agent\necho \"Building finance agent...\"\ndocker build -t finance-boardroom-tee:latest -f spoke_finance/Dockerfile .\n\n# Build marketing agent\necho \"Building marketing agent...\"\ndocker build -t marketing-boardroom-tee:latest -f spoke_marketing/Dockerfile .\n\necho \"All images built successfully!\"\n\n# List built images\ndocker images | grep boardroom\n```\n\n---\n\n## Memory Optimization Strategies\n\n### Model Loading Optimization\n```python\n# src/shared/memory_optimization.py\nimport torch\nimport gc\nimport psutil\nfrom typing import Dict\n\nclass MemoryOptimizer:\n    \"\"\"Memory optimization utilities for large models\"\"\"\n    \n    @staticmethod\n    def optimize_model_loading(model_name: str, max_memory_mb: int) -> Dict:\n        \"\"\"Optimize model loading based on available memory\"\"\"\n        available_memory = psutil.virtual_memory().available / 1024 / 1024\n        \n        config = {\n            \"torch_dtype\": torch.float16,\n            \"device_map\": \"cpu\",\n            \"low_cpu_mem_usage\": True,\n            \"use_cache\": False\n        }\n        \n        # Add quantization for very large models\n        if \"7B\" in model_name and available_memory < max_memory_mb * 0.8:\n            config[\"load_in_4bit\"] = True\n            config[\"bnb_4bit_compute_dtype\"] = torch.float16\n        \n        return config\n    \n    @staticmethod\n    def cleanup_memory():\n        \"\"\"Force memory cleanup\"\"\"\n        gc.collect()\n        torch.cuda.empty_cache() if torch.cuda.is_available() else None\n    \n    @staticmethod\n    def get_memory_stats() -> Dict:\n        \"\"\"Get current memory statistics\"\"\"\n        memory = psutil.virtual_memory()\n        return {\n            \"total_mb\": memory.total / 1024 / 1024,\n            \"available_mb\": memory.available / 1024 / 1024,\n            \"used_mb\": memory.used / 1024 / 1024,\n            \"percent_used\": memory.percent\n        }\n```\n\n---\n\n## Deployment Instructions\n\n### Step 1: Prepare Environment\n```bash\n# Clone repository and navigate to boardroom_tee\ncd F:/coding/boardroom_tee\n\n# Create necessary directories\nmkdir -p base-agent/src/shared\nmkdir -p base-agent/src/attestation  \nmkdir -p base-agent/src/communication\nmkdir -p hub/src/hub\nmkdir -p spoke_finance/src/agents/finance\nmkdir -p spoke_marketing/src/agents/marketing\n```\n\n### Step 2: Build Images\n```bash\n# Make build script executable\nchmod +x build.sh\n\n# Build all images\n./build.sh\n```\n\n### Step 3: Deploy Services\n```bash\n# Deploy hub\ncd hub\ndocker-compose up -d\n\n# Deploy finance agent\ncd ../spoke_finance\ndocker-compose up -d\n\n# Deploy marketing agent\ncd ../spoke_marketing\ndocker-compose up -d\n```\n\n### Step 4: Verify Deployment\n```bash\n# Check all services\ndocker ps\ncurl http://localhost:8080/health  # Hub\ncurl http://localhost:8081/health  # Finance\ncurl http://localhost:8082/health  # Marketing\n```\n\n---\n\n*Last Updated: [Current Date]*  \n*Related: spoke_agents_premium.md, hub_data_processing.md, distributed_attestation_plan.md*  \n*Version: 1.0 - Transformers Implementation*